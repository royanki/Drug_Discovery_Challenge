# Comprehensive Stacking Pipeline

A modular, extensible pipeline for stacking ensemble learning with molecular fingerprints, featuring **final training on complete data** and **intelligent submission file generation** for drug discovery challenges.

## ğŸ¯ New Features

- **ğŸš€ Two-Phase Workflow**: Cross-validation for model selection + Final training for submissions
- **ğŸ§ª Chemical Diversity Selection**: RDKit-based Tanimoto similarity clustering with preprocessed fingerprints
- **ğŸ“Š Intelligent Submission Generation**: Top confident + chemically diverse predictions
- **âš¡ Memory-Efficient Processing**: One fingerprint at a time with zero-variance removal
- **ğŸ”„ Preprocessed Fingerprint Consistency**: Same preprocessing for modeling and diversity

## Installation

```bash
# Install required packages
pip install pandas numpy scikit-learn xgboost matplotlib seaborn rdkit-pypi

# âš ï¸ **IMPORTANT**: RDKit is now required for chemical diversity selection
# If installation fails, try: conda install -c conda-forge rdkit

# Clone or download the pipeline
git clone <your-repo-url>
cd stacking_pipeline
```

## ğŸ”¥ Quick Start - Two-Phase Workflow

### **Phase 1: Cross-Validation (Model Selection)**

```bash
# Run CV with XGBoost to find best ensemble strategy
python -m stacking_pipeline.main --base-learner xgboost --output results/xgb_cv_results.pkl

# Run CV with Random Forest
python -m stacking_pipeline.main --base-learner random_forest --output results/rf_cv_results.pkl

# Run CV with Neural Network  
python -m stacking_pipeline.main --base-learner neural_net --output results/nn_cv_results.pkl
```

### **Phase 2: Final Training & Submission Generation**

```bash
# ğŸ¯ Use XGBoost CV results for final training on complete data
python -m stacking_pipeline.main --final-train \
               --cv-results results/xgb_cv_results.pkl \
               --test-data Data/challenge_test_set.parquet \
               --submission-dir results/xgb_submissions

# ğŸ¯ Use Random Forest CV results  
python -m stacking_pipeline.main --final-train \
               --cv-results results/rf_cv_results.pkl \
               --test-data Data/challenge_test_set.parquet \
               --submission-dir results/rf_submissions
```

> **ğŸ”´ CRITICAL**: <span style='color:red'>The pipeline uses relative imports and **MUST** be run as a module with `python -m stacking_pipeline.main`. Running `python main.py` directly will cause ImportError.</span>

## ğŸ“ Generated Submission Files

The final training generates **4 submission files**:

```
results/xgb_submissions/
â”œâ”€â”€ top_200_xgboost.csv        # ğŸ† Top 200 most confident predictions
â”œâ”€â”€ top_500_xgboost.csv        # ğŸ† Top 500 most confident predictions  
â”œâ”€â”€ diverse_200_xgboost.csv    # ğŸ§¬ 200 chemically diverse predictions
â””â”€â”€ all_predictions_xgboost.csv # ğŸ“‹ All predictions ranked by confidence
```

### ğŸ§¬ Chemical Diversity Selection Features

- **ğŸ”¬ Uses ALL 9 fingerprints**: ECFP4, ECFP6, FCFP4, FCFP6, TOPTOR, MACCS, RDK, AVALON, ATOMPAIR
- **âš™ï¸ Consistent Preprocessing**: Same zero-variance removal as model training
- **ğŸ¯ RDKit Tanimoto Clustering**: Butina algorithm for proper chemical diversity
- **ğŸ›¡ï¸ Robust Fallback**: Building block diversity if RDKit fails
- **ğŸ“Š Detailed Logging**: Complete transparency of selection process

## âš ï¸ **CRITICAL**: Test Data Path Requirements

> **ğŸ”´ WARNING**: You **MUST** specify the test data path when running final training!

### **Option 1: Command Line (Recommended)**
```bash
python main.py --final-train \
               --cv-results results/cv_results.pkl \
               --test-data Data/YOUR_TEST_FILE.parquet  # ğŸ”´ REQUIRED!
```

### **Option 2: Default Configuration**
```python
def create_default_config():
    return {
        'data_path': "Data/WDR91.parquet",
        'test_data_path': "Data/YOUR_TEST_FILE.parquet",  # Add this line
        # ... other config
    }
```

### **Option 3: JSON Configuration File**
```json
{
    "data_path": "Data/WDR91.parquet",
    "test_data_path": "Data/YOUR_TEST_FILE.parquet",
    "fingerprint_cols": ["ECFP4", "ECFP6", "FCFP4", "FCFP6", "TOPTOR", "MACCS", "RDK", "AVALON", "ATOMPAIR"]
}
```

## ğŸš€ Complete Workflow Example

```bash
# 1ï¸âƒ£ Run cross-validation (find best strategy & weights)
python -m stacking_pipeline.main --base-learner xgboost \
               --output results/xgb_cv_results.pkl \
               --folds 5

# 2ï¸âƒ£ Train final models on complete data & generate submissions
python -m stacking_pipeline.main --final-train \
               --cv-results results/xgb_cv_results.pkl \
               --test-data Data/challenge_test_set.parquet \
               --submission-dir results/final_submissions \
               --output results/final_results.pkl

# 3ï¸âƒ£ Check your submission files!
ls results/final_submissions/
```

## âš™ï¸ Advanced Configuration

### **Meta-Learner Selection**
> **ğŸŸ¡ NOTE**: Currently uses Logistic Regression + Random Forest meta-learners only
> 
> KNN and MLP meta-learners are excluded for performance reasons

### **Ensemble Strategies**
Available strategies (automatically compared during CV):

- `weighted`: ğŸ¯ Optimized weighted combination
- `threshold`: ğŸ”„ Threshold-based hybrid approach  
- `calibrated`: ğŸ“Š Calibrated probability ensemble
- `robust`: ğŸ›¡ï¸ Robust validation-based combination

### **Memory-Efficient Processing**
```python
# The pipeline processes one fingerprint at a time to save memory
# Each fingerprint undergoes zero-variance removal before modeling
# Same preprocessing is applied for diversity calculation
```

## ğŸ“Š Understanding CV Results

The CV phase saves critical information for final training:

```python
cv_results = {
    'cv_summary': {
        'best_ensemble_strategy': 'Ensemble_weighted',
        'optimal_weights': {'LogisticRegression': 0.3, 'RandomForest': 0.7},
        'fingerprint_meta_cols': ['ECFP4_xgboost', 'ECFP6_xgboost', ...],
        'best_metrics': {'F1-Score': 0.8234, 'AUC-ROC': 0.9156, ...}
    }
}
```

## ğŸ”§ File Structure (Updated)

```
project_root/
â”œâ”€â”€ stacking_pipeline/
â”‚   â”œâ”€â”€ __init__.py               # Package initialization
â”‚   â”œâ”€â”€ main.py                   # ğŸ¯ Main pipeline with two-phase workflow
â”‚   â”œâ”€â”€ final_train_predict.py    # ğŸ†• Final training & prediction module
â”‚   â”œâ”€â”€ base_learners.py          # Base learner configurations
â”‚   â”œâ”€â”€ meta_learners.py          # Meta-learner training (LR + RF only)
â”‚   â”œâ”€â”€ ensemble_strategies.py    # Meta-learner combination strategies
â”‚   â”œâ”€â”€ analysis.py               # Analysis and visualization
â”‚   â”œâ”€â”€ utils.py                  # Helper functions
â”‚   â”œâ”€â”€ preprocessing.py          # Data preprocessing with zero-variance removal
â”‚   â””â”€â”€ disynthon_split.py        # Custom split function (user-provided)
â”œâ”€â”€ requirements.txt              # Dependencies list
â”œâ”€â”€ Data/                         # Data directory
â”‚   â”œâ”€â”€ WDR91.parquet            # Training data
â”‚   â””â”€â”€ test_set.parquet         # Test data
â””â”€â”€ results/                     # Output directory
    â”œâ”€â”€ cv_results.pkl           # CV results
    â””â”€â”€ submissions/             # Final submission files
```

> **ğŸ”´ CRITICAL**: The pipeline is a Python package with relative imports. Always run with `python -m stacking_pipeline.main` from the project root directory.

## ğŸ¯ Performance Metrics

The pipeline reports comprehensive metrics:

- **ğŸ“Š Standard Metrics**: Precision, Recall, F1-Score, AUC-ROC
- **ğŸ¯ Challenge-Specific**: PPV@128, PPV@256, Hits@128, Hits@256
- **ğŸ“ˆ Advanced**: PR AUC, Calibration metrics
- **ğŸ§¬ Diversity Metrics**: Chemical diversity statistics

## âš ï¸ **WARNINGS & TROUBLESHOOTING**

### **ğŸ”´ Critical Issues**

1. **Missing RDKit**: 
   ```bash
   # If you get import errors for RDKit:
   conda install -c conda-forge rdkit
   # or
   pip install rdkit-pypi
   ```

2. **Import Errors with Relative Paths**:
   ```
   ImportError: attempted relative import with no known parent package
   ```
   **Solution**: The pipeline uses absolute imports. Run `python main.py` directly from the project root directory where all Python files are located.

3. **Test Data Path Not Found**:
   ```
   Error: Test data file not found: Data/test_set.parquet
   ```
   **Solution**: Verify your test file path is correct!

4. **CV Results Missing**:
   ```
   Error: --cv-results path is required when using --final-train
   ```
   **Solution**: Run CV phase first to generate .pkl file!

### **ğŸŸ¡ Performance Warnings**

1. **Memory Issues**: 
   - Reduce fingerprints: `--fingerprint-subset ['ECFP4', 'ECFP6']`
   - Reduce CV folds: `--folds 3`

2. **Slow Diversity Calculation**:
   - Uses top 1000 candidates by default
   - Falls back to building block diversity if RDKit fails

### **ğŸŸ¢ Best Practices**

1. **ğŸ¯ Start with CV**: Always run cross-validation first
2. **ğŸ’¾ Save Results**: Use `--output` to save CV results  
3. **ğŸ§ª Test Different Base Learners**: Compare XGBoost vs Random Forest vs Neural Net
4. **ğŸ“Š Check Diversity**: Ensure diverse_200.csv has good chemical diversity
5. **ğŸ” Validate Submissions**: Check that submission files have expected formats
6. **ğŸ“ Project Structure**: Keep all Python files in the same directory and run from project root

## ğŸ†• New Command Line Options

```bash
# CV Phase Options
python main.py --base-learner xgboost --output results.pkl --folds 5 --save-vis

# Final Training Phase Options  
python main.py --final-train \
               --cv-results results.pkl \        # ğŸ”´ REQUIRED
               --test-data test.parquet \        # ğŸ”´ REQUIRED  
               --submission-dir submissions/ \   # Optional (default: results/submissions)
               --output final_results.pkl       # Optional

# Utility Options
--no-vis              # Disable visualizations
--save-vis            # Save visualizations to files
--folds N             # Number of CV folds (default: 5)
```

## ğŸ§¬ Chemical Diversity Technical Details

### **Preprocessing Consistency**
> **ğŸŸ¢ IMPORTANT**: Diversity calculation uses the **same preprocessed fingerprints** as model training
> 
> - Zero-variance bits removed per fingerprint
> - ECFP4: ~1847 bits (after removing ~201 zero-variance)
> - MACCS: ~167 bits (after removing many zero-variance)
> - Combined fingerprint: ~8000-12000 total informative bits

### **Diversity Algorithm**
1. **Candidate Selection**: Top 1000 predictions by confidence
2. **Fingerprint Combination**: Concatenate all 9 preprocessed fingerprints
3. **Similarity Calculation**: RDKit Tanimoto similarity
4. **Clustering**: Butina algorithm (threshold=0.7)
5. **Representative Selection**: Highest confidence from each cluster
6. **Gap Filling**: Add high-confidence molecules if needed

## ğŸ“‹ Quick Reference

### **Typical Workflow**
```bash
# 1. CV Phase (once per base learner)
python -m stacking_pipeline.main --base-learner xgboost --output xgb_cv.pkl

# 2. Final Phase (generates submissions)  
python -m stacking_pipeline.main --final-train --cv-results xgb_cv.pkl --test-data test.parquet

# 3. Submit files from results/submissions/
```

### **File Formats**
All CSV files contain:
- `BB1_ID`, `BB2_ID`: Building block identifiers
- `Prediction_Probability`: Model confidence score
- `Confidence_Rank` or `Diversity_Rank`: Selection ranking
- `Selection_Method`: How molecule was selected

## ğŸ‰ Ready for Drug Discovery Challenges!

This updated pipeline provides everything needed for modern drug discovery competitions:

âœ… **Two-phase workflow** for optimal model selection and final training  
âœ… **Chemical diversity selection** using state-of-the-art RDKit methods  
âœ… **Memory-efficient processing** of large molecular datasets  
âœ… **Consistent preprocessing** between modeling and diversity calculation  
âœ… **Comprehensive submission formats** (confident + diverse predictions)  
âœ… **Robust error handling** with meaningful warnings and fallbacks  

> **ğŸ¯ Success Tip**: Run CV on a subset of fingerprints first to find the best setup, then run final training with all fingerprints for maximum performance!